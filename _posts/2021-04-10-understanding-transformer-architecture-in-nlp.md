---
id: 641
title: 'Resources to Understand Transformer Architecture in NLP'
date: '2021-04-10T03:06:59+00:00'
author: 'Asif Rehan'
layout: post
guid: 'https://asifrehan.com/?p=641'
permalink: /understanding-transformer-architecture-in-nlp/
categories:
    - Algorithms
    - 'Data Science'
tags:
    - Attention
    - 'Data Science'
    - 'Deep Learning'
    - Google
    - 'Machine Learning'
    - 'Natural Language Processing'
    - NLP
    - 'Recurrent Neural Network'
    - Research
    - RNN
    - Transformer
---

Posting some great resources to understand the Transformer architecture for NLP presented in the paper “Attention is All You Need” ([Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)).

1. This [website by J Al-Ammar](http://jalammar.github.io/illustrated-transformer/) is excellent
2. The next best resource is this [annotated implementation of Transformer in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard University
3. Second, read this article called [“Attention! Attention!” by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model)
4. For further background on Word Embeddings, look into [this post](https://machinelearningmastery.com/what-are-word-embeddings/) by Jason Brownlee.