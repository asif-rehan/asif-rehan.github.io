---
id: 673
title: 'Resources to Understand Transformer Architecture in NLP'
date: '2021-04-24T19:07:05+00:00'
author: 'Asif Rehan'
layout: revision
guid: 'https://asifrehan.com/641-revision-v1/'
permalink: /641-revision-v1/
---

Posting some great resources to understand the Transformer architecture for NLP presented in the paper “Attention is All You Need” ([Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)).

1. This [website by J Al-Ammar](http://jalammar.github.io/illustrated-transformer/) is excellent
2. The next best resource is this [annotated implementation of Transformer in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html) from Harvard University
3. Second, read this article called [“Attention! Attention!” by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model)
4. For further background on Word Embeddings, look into [this post](https://machinelearningmastery.com/what-are-word-embeddings/) by Jason Brownlee.