---
id: 642
title: 'Understanding Transformer Architecture in NLP'
date: '2021-04-10T03:06:59+00:00'
author: 'Asif Rehan'
layout: revision
guid: 'https://asifrehan.com/641-revision-v1/'
permalink: /641-revision-v1/
---

Posting some great resources to understand the Transformer architecture for NLP presented in the paper “Attention is All You Need” ([Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762.pdf)).

1. This [website by J Al-Ammar](http://jalammar.github.io/illustrated-transformer/) is excellent
2. Second, read this article called [“Attention! Attention!” by Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#whats-wrong-with-seq2seq-model)
3. For further background on Word Embeddings, look into [this post](https://machinelearningmastery.com/what-are-word-embeddings/) by Jason Brownlee.